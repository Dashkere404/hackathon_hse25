{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ –•–∞–∫–∞—Ç–æ–Ω: –ó–∞–¥–∞–Ω–∏–µ 1. –¢—é–Ω–∏–Ω–≥ –∏ –û—Ü–µ–Ω–∫–∞ LLM\n",
        "\n",
        "–í—Å–µ–º –ø—Ä–∏–≤–µ—Ç! –ù–∞—à–∞ –∫–æ–º–∞–Ω–¥–∞ –∑–∞–Ω–∏–º–∞–µ—Ç—Å—è —Å–æ–∑–¥–∞–Ω–∏–µ–º —á–∞—Ç-–±–æ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ RAG (https://habr.com/ru/articles/779526/) –¥–ª—è –ø–æ–º–æ—â–∏ —Å—Ç—É–¥–µ–Ω—Ç–∞–º –ù–ò–£ –í–®–≠ —Å –æ—Ç–≤–µ—Ç–∞–º–∏ –Ω–∞ –∏—Ö –≤–æ–ø—Ä–æ—Å—ã, –∫–∞—Å–∞—é—â–∏—Ö—Å—è —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–∞.\n",
        "\n",
        "–î–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π —Ä–∞–±–æ—Ç—ã –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏ –Ω–∞–º –∫–∞–∂–¥—ã–π —Ä–∞–∑ –ø—Ä–∏—Ö–æ–¥–∏—Ç—Å—è –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å –µ–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç. –û–∑–Ω–∞–∫–æ–º–∏—Ç—å—Å—è —Å –Ω–∏–º –º–æ–∂–µ—Ç–µ –≤ —Ñ–∞–π–ª–µ system_prompt.yaml.\n",
        "\n",
        "–ö–∞–∂–¥—ã–π —Ä–∞–∑ –µ–≥–æ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å –≤ –º–æ–¥–µ–ª—å –Ω–µ —Ö–æ—á–µ—Ç—Å—è, –ø–æ—ç—Ç–æ–º—É –Ω–∞–º –±—ã —Ö–æ—Ç–µ–ª–æ—Å—å –∏–º–µ—Ç—å –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è *—É–∂–µ –±—É–¥–µ—Ç –Ω–∞—Ç—é–Ω–µ–Ω–∞ —Å–ª–µ–¥–æ–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º*, –ø—Ä–æ–ø–∏—Å–∞–Ω–Ω—ã–º –≤ —Å–∏—Å—Ç–µ–º–Ω–æ–º –ø—Ä–æ–º–ø—Ç–µ –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —É–∫–∞–∑–∞–Ω–∏–π."
      ],
      "metadata": {
        "id": "blLQSC_cOKSq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**–£—Å–ª–æ–≤–∏—è –∏ –∫–∞–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –º—ã –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ–º**"
      ],
      "metadata": {
        "id": "_3oR-fADQ7We"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> –£—Å–ª–æ–≤–∏—è\n",
        "\n",
        "\n",
        "1.   –ù–∞—Ç—é–Ω—å—Ç–µ –ø–æ–±–æ–ª—å—à–µ –ª–µ–≥–∫–æ–≤–µ—Å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –ù–∏–∂–µ –µ—Å—Ç—å –∫–æ–¥ –¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Ä–∞–∑–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤, –ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥–∏–µ, –ø—Ä–æ–±—É–π—Ç–µ —Ç–µ, —á—Ç–æ —É–∫–∞–∑–∞–Ω—ã –≤ –∫–æ–¥–µ (–ø—Ä–æ —Ç—é–Ω–∏–Ω–≥, –µ–≥–æ –≤–∏–¥—ã –∏ –ø—Ä–æ—á–µ–µ –º–æ–∂–µ—Ç–µ –ø–æ—á–∏—Ç–∞—Ç—å –∑–¥–µ—Å—å –∏ –∑–¥–µ—Å—å.) [–ü—Ä–æ—Å—Ç–æ –ø—Ä–∏–∫–æ–ª—å–Ω–∞—è –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è –ø—Ä–æ –ª–ª–º–∫–∏, –≤–∏–¥—ã –æ–±—É—á–µ–Ω–∏—è, –¥–æ–æ–±—É—á–µ–Ω–∏—è, –±–µ–Ω—á–º–∞—Ä–∫–∏](https://mellain.github.io/data/YNDX_Meetup_LLM_Train_Slides_2024.pdf)\n",
        "\n",
        "2.   –û—Ç–≤–∞–ª–∏–¥–∏—Ä—É–π—Ç–µ –º–æ–¥–µ–ª—å (–ø–æ–¥–æ–±—Ä–∞—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –Ω–∞ —Ç–æ, —á—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç —Å –ª–æ—Å—Å–æ–º). –°–æ–≤–µ—Ç—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å [wandb](https://wandb.ai/)\n",
        "\n",
        "3. –û—Ç–æ–±—Ä–∞–∑–∏—Ç—å –≤ –∫—Ä–∞—Å–∏–≤–æ–º, –ø–æ–Ω—è—Ç–Ω–æ–º, –∏–Ω—Ç–µ–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ–º –≤–∏–¥–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã‚ò∫Ô∏è‚ò∫Ô∏è‚ò∫Ô∏è\n",
        "\n",
        "P.S. –Ω–µ –∑–∞–±—É–¥—å—Ç–µ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ–¥–Ω—É –ª—É—á—à—É—é –º–æ–¥–µ–ª—å, –ø–æ –∏—Ç–æ–≥–∞–º –º—ã –ø—Ä–æ–≥–æ–Ω–∏–º –≤–∞—à–∏ –º–æ–¥–µ–ª–∏ –Ω–∞ —Å–∫—Ä—ã—Ç–æ–π –≤—ã–±–æ—Ä–∫–µ –∏ –æ–ø—Ä–µ–¥–µ–ª–∏–º –ø–æ–±–µ–¥–∏—Ç–µ–ª–µ–π‚ò†Ô∏è\n",
        "\n",
        "\n",
        "> –ß—Ç–æ –º—ã –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ–º"
      ],
      "metadata": {
        "id": "ia3AlRP5Rg3W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**–ö–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç –±—ã—Ç—å –ø–æ–ª–µ–∑–µ–Ω :)**"
      ],
      "metadata": {
        "id": "Q--8Zi4-RAVy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###–í—Å–µ –∏–º–ø–æ—Ä—Ç—ã"
      ],
      "metadata": {
        "id": "SqNuouv-SWoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%pip install transformers datasets accelerate rouge-score nltk bitsandbytes peft"
      ],
      "metadata": {
        "id": "D_s8Ekpi3o97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import torch\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from rouge import Rouge\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "bhQgCGRqSVwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"
      ],
      "metadata": {
        "id": "7g9kTa_NRicD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A6TKBPEtRNk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### –ö–æ–¥ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (–º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª—é–±—ã–µ, –Ω–æ –Ω–µ –±–µ—Ä–∏—Ç–µ —Å–ª–∏—à–∫–æ–º —Ç—è–∂–µ–ª–æ–≤–µ—Å–Ω—ã–µ, –Ω–∞ –±–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –≥–ø—É —Å –∫–æ–ª–∞–±–∞ –≤—ã –Ω–µ —É—Å–ø–µ–µ—Ç–µ –µ–µ –æ–±—É—á–∏—Ç—å :)))"
      ],
      "metadata": {
        "id": "MdLy7dN7Rwve"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### –ö–ª–∞—Å—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"
      ],
      "metadata": {
        "id": "eC-vZYpU4CF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelTrainer:\n",
        "    def __init__(self, model_name: str, task: str = \"causal\", quantization: bool = True):\n",
        "        self.model_name = model_name\n",
        "        self.task = task\n",
        "        self.quantization = quantization\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.model, self.tokenizer = self.load_model()\n",
        "\n",
        "    def load_model(self):\n",
        "        kwargs = {\"load_in_4bit\": True, \"device_map\": \"auto\"} if self.quantization else {\"device_map\": \"auto\"}\n",
        "\n",
        "        if self.task == \"causal\":\n",
        "            model = AutoModelForCausalLM.from_pretrained(self.model_name, **kwargs)\n",
        "        elif self.task == \"classification\":\n",
        "            model = AutoModelForSequenceClassification.from_pretrained(self.model_name, **kwargs)\n",
        "        else:\n",
        "            raise ValueError(\"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–∞—è –∑–∞–¥–∞—á–∞\")\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "        model.to(self.device)\n",
        "        return model, tokenizer\n",
        "\n",
        "    def load_data(self, data_path: str):\n",
        "        dataset = load_dataset(\"json\", data_files=data_path)\n",
        "        return dataset.map(lambda examples: self.tokenizer(examples[\"question\"] + \" \" + examples.get(\"context\", \"\"),\n",
        "                                                      truncation=True, padding=\"max_length\", max_length=512),\n",
        "                                                      batched=True)\n",
        "\n",
        "    def train(self, train_data_path: str, output_dir: str = \"./results\", epochs: int = 3, batch_size: int = 2):\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        tokenized_datasets = self.load_data(train_data_path)\n",
        "        train_dataloader = torch.utils.data.DataLoader(tokenized_datasets[\"train\"], batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        self.model.train()\n",
        "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=5e-5)\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=True)\n",
        "\n",
        "            for batch in progress_bar:\n",
        "                batch = {k: v.to(self.model.device) for k, v in batch.items()}\n",
        "                optimizer.zero_grad()\n",
        "                outputs = self.model(**batch)\n",
        "                loss = outputs.loss\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                progress_bar.set_postfix(loss=loss.item())\n",
        "\n",
        "        self.model.save_pretrained(output_dir)\n",
        "        print(f\"–ú–æ–¥–µ–ª—å {self.model_name} —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {output_dir}.\")"
      ],
      "metadata": {
        "id": "OzzuOeV8SCtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### –û–±—É—á–∞–µ–º"
      ],
      "metadata": {
        "id": "ts2-P1yX36z8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "available_models = {\n",
        "    \"causal\": [\"ai-forever/saiga_mistral_7b\", \"mistralai/Mistral-7B-Instruct\", \"SberbankAI/FRED-T5-1.7B\"],\n",
        "    \"classification\": [\"distilbert-base-uncased\", \"bert-tiny\"]\n",
        "} #–≥—É–≥–ª–∏—Ç–µ –ª—é–±—ã–µ, —ç—Ç–æ –ø—Ä–æ—Å—Ç–æ –ø—Ä–∏–º–µ—Ä—É—Å\n",
        "\n",
        "selected_model = \"ai-forever/saiga_mistral_7b\"\n",
        "trainer = ModelTrainer(selected_model)\n",
        "trainer.train(\"train_data.json\")"
      ],
      "metadata": {
        "id": "MwzKcN543_BI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###–ö–æ–¥ –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ –º–µ—Ç—Ä–∏–∫ –≤–∞—à–µ–π –º–æ–¥–µ–ª—å–∫–∏ :)"
      ],
      "metadata": {
        "id": "Egz3ldjcSGjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_rouge(reference, hypothesis):\n",
        "    rouge = Rouge()\n",
        "    scores = rouge.get_scores(hypothesis, reference)\n",
        "    return scores[0][\"rouge-l\"][\"f\"]\n",
        "\n",
        "def compute_bleu(reference, hypothesis):\n",
        "    return sentence_bleu([reference.split()], hypothesis.split())\n",
        "\n",
        "def compute_perplexity(model, tokenizer, text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
        "        loss = outputs.loss\n",
        "    return math.exp(loss.item())"
      ],
      "metadata": {
        "id": "yAZ-2KreSNWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = json.load(open(\"generated_answers.json\", \"r\", encoding=\"utf-8\"))\n",
        "\n",
        "rouge_scores, bleu_scores, perplexity_scores = [], [], []\n",
        "\n",
        "for item in test_data:\n",
        "    ref = item[\"gold_answer\"]\n",
        "    hyp = item[\"generated_answer\"]\n",
        "\n",
        "    rouge_scores.append(compute_rouge(ref, hyp))\n",
        "    bleu_scores.append(compute_bleu(ref, hyp))\n",
        "    perplexity_scores.append(compute_perplexity(model, tokenizer, hyp))\n",
        "\n",
        "print(f\"–°—Ä–µ–¥–Ω–∏–π ROUGE-L: {sum(rouge_scores) / len(rouge_scores)}\")\n",
        "print(f\"–°—Ä–µ–¥–Ω–∏–π BLEU: {sum(bleu_scores) / len(bleu_scores)}\")\n",
        "print(f\"–°—Ä–µ–¥–Ω–∏–π Perplexity: {sum(perplexity_scores) / len(perplexity_scores)}\")"
      ],
      "metadata": {
        "id": "-Hcp2X9H4h21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###–ì—Ä–∞—Ñ–∏—á–∫–∏, —à–æ–± –∫—Ä–∞—Å–∏–≤–æ :)"
      ],
      "metadata": {
        "id": "ysErAoYj4aBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_metrics(rouge_scores, bleu_scores, perplexity_scores):\n",
        "    indices = np.arange(len(rouge_scores))\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(indices, rouge_scores, marker='o', linestyle='-', label='ROUGE-L')\n",
        "    plt.xlabel(\"–ù–∞–±–ª—é–¥–µ–Ω–∏—è\")\n",
        "    plt.ylabel(\"–ú–µ—Ç—Ä–∏–∫–∞\")\n",
        "    plt.title(\"ROUGE-L –º–µ—Ç—Ä–∏–∫–∞\")\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.plot(indices, bleu_scores, marker='s', linestyle='-', label='BLEU', color='orange')\n",
        "    plt.xlabel(\"–ù–∞–±–ª—é–¥–µ–Ω–∏—è\")\n",
        "    plt.ylabel(\"–ú–µ—Ç—Ä–∏–∫–∞\")\n",
        "    plt.title(\"BLEU –º–µ—Ç—Ä–∏–∫–∞\")\n",
        "    plt.grid(True)\n",
        "\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.plot(indices, perplexity_scores, marker='^', linestyle='-', label='Perplexity', color='red')\n",
        "    plt.xlabel(\"–ù–∞–±–ª—é–¥–µ–Ω–∏—è\")\n",
        "    plt.ylabel(\"–ú–µ—Ç—Ä–∏–∫–∞\")\n",
        "    plt.title(\"Perplexity –º–µ—Ç—Ä–∏–∫–∞\")\n",
        "    plt.grid(True)\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_metrics(rouge_scores, bleu_scores, perplexity_scores)\n"
      ],
      "metadata": {
        "id": "XSnXC4xW17lh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}